{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import math\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfrom glob import glob\n%matplotlib inline\nimport matplotlib.pyplot as plt\n#!pip install  chart_studio\nimport seaborn as sns\n\nfrom functools import reduce\nfrom glob import glob\n\n#ipywidgets for some interactive plots\nfrom ipywidgets.widgets import * \nimport ipywidgets as widgets\n\n#plotly 3D interactive graphs \nimport plotly\nfrom plotly.graph_objs import *\n#import chart_studio\n\nimport random\nfrom random import sample\nimport sklearn.model_selection as skl\nfrom sklearn.model_selection import train_test_split\nfrom kaggle_datasets import KaggleDatasets\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom keras_preprocessing.image.dataframe_iterator import DataFrameIterator\nfrom tensorflow.keras.applications import EfficientNetB3\nfrom tensorflow.keras.layers import InputLayer, GlobalAveragePooling2D, BatchNormalization, Dense, Dropout, Flatten, Conv2D, MaxPooling2D \nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.metrics import AUC, MeanAbsoluteError, BinaryCrossentropy, BinaryAccuracy\nfrom tensorflow.keras.optimizers import RMSprop, SGD\nfrom tensorflow.keras.losses import BinaryCrossentropy, MeanAbsoluteError ,Hinge\nimport tensorflow_hub as tfhub\nfrom tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nimport pydicom\nimport cv2","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-29T07:32:41.090293Z","iopub.execute_input":"2022-05-29T07:32:41.090766Z","iopub.status.idle":"2022-05-29T07:32:49.308552Z","shell.execute_reply.started":"2022-05-29T07:32:41.090684Z","shell.execute_reply":"2022-05-29T07:32:49.307373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Forming the Data Frames**\n**In the following cells we have split the train data set into train, validation and test.\nWe have done so because our dataset was part of a competition and we had no means of checking our model's accuracy with the test dataset as it had no labels.**","metadata":{}},{"cell_type":"code","source":"root_dir = '../input/rsna-miccai-brain-tumor-radiogenomic-classification/'\ndf = pd.read_csv(root_dir+'train_labels.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-29T07:32:49.312692Z","iopub.execute_input":"2022-05-29T07:32:49.312994Z","iopub.status.idle":"2022-05-29T07:32:49.351684Z","shell.execute_reply.started":"2022-05-29T07:32:49.312962Z","shell.execute_reply":"2022-05-29T07:32:49.350501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add the full paths for each id for different types of sequences to the csv \ndef full_ids(data):\n    zeros = 5 - len(str(data))\n    if zeros > 0:\n        prefix = ''.join(['0' for i in range(zeros)])\n    \n    return prefix+str(data)\n        \n\ndf['BraTS21ID_full'] = df['BraTS21ID'].apply(full_ids)\n\n# Add all the paths to the df for easy access\ndf['flair'] = df['BraTS21ID_full'].apply(lambda file_id : root_dir+'train/'+file_id+'/FLAIR/')\ndf['t1w'] = df['BraTS21ID_full'].apply(lambda file_id : root_dir+'train/'+file_id+'/T1w/')\ndf['t1wce'] = df['BraTS21ID_full'].apply(lambda file_id : root_dir+'train/'+file_id+'/T1wCE/')\ndf['t2w'] = df['BraTS21ID_full'].apply(lambda file_id : root_dir+'train/'+file_id+'/T2w/')\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-29T07:32:49.354075Z","iopub.execute_input":"2022-05-29T07:32:49.354609Z","iopub.status.idle":"2022-05-29T07:32:49.387986Z","shell.execute_reply.started":"2022-05-29T07:32:49.354549Z","shell.execute_reply":"2022-05-29T07:32:49.386874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df, df_test = train_test_split(df,test_size=0.1)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T07:32:49.390108Z","iopub.execute_input":"2022-05-29T07:32:49.390842Z","iopub.status.idle":"2022-05-29T07:32:49.400656Z","shell.execute_reply.started":"2022-05-29T07:32:49.390796Z","shell.execute_reply":"2022-05-29T07:32:49.399505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-29T07:32:49.40217Z","iopub.execute_input":"2022-05-29T07:32:49.40291Z","iopub.status.idle":"2022-05-29T07:32:49.420805Z","shell.execute_reply.started":"2022-05-29T07:32:49.402865Z","shell.execute_reply":"2022-05-29T07:32:49.41938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-29T07:32:49.422843Z","iopub.execute_input":"2022-05-29T07:32:49.423811Z","iopub.status.idle":"2022-05-29T07:32:49.441521Z","shell.execute_reply.started":"2022-05-29T07:32:49.423684Z","shell.execute_reply":"2022-05-29T07:32:49.439955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2022-05-29T07:32:49.443819Z","iopub.execute_input":"2022-05-29T07:32:49.444361Z","iopub.status.idle":"2022-05-29T07:32:49.472714Z","shell.execute_reply.started":"2022-05-29T07:32:49.444316Z","shell.execute_reply":"2022-05-29T07:32:49.47108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.describe()","metadata":{"execution":{"iopub.status.busy":"2022-05-29T07:32:49.477115Z","iopub.execute_input":"2022-05-29T07:32:49.47748Z","iopub.status.idle":"2022-05-29T07:32:49.498498Z","shell.execute_reply.started":"2022-05-29T07:32:49.477441Z","shell.execute_reply":"2022-05-29T07:32:49.497115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.groupby('MGMT_value').count().head()","metadata":{"execution":{"iopub.status.busy":"2022-05-29T07:32:49.501231Z","iopub.execute_input":"2022-05-29T07:32:49.502093Z","iopub.status.idle":"2022-05-29T07:32:49.520951Z","shell.execute_reply.started":"2022-05-29T07:32:49.502041Z","shell.execute_reply":"2022-05-29T07:32:49.519442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.groupby('MGMT_value').count().head()","metadata":{"execution":{"iopub.status.busy":"2022-05-29T07:32:49.523081Z","iopub.execute_input":"2022-05-29T07:32:49.523782Z","iopub.status.idle":"2022-05-29T07:32:49.540756Z","shell.execute_reply.started":"2022-05-29T07:32:49.523734Z","shell.execute_reply":"2022-05-29T07:32:49.53938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading the Images\n\n**In the following cells:**\n* We are actually splitting the dataset into train, validation and test and we are organising the respective labels\n* We are augmenting the data in real time to bring about some variation in the dataset\n* We are also using our user defined class to itterate through the dataframe\n\n**Our dataset had a few discrepencies with a few images such as:**\n* 00109 (FLAIR images are blank)\n* 00123 (T1w images are blank)\n* 00709 (FLAIR images are blank)","metadata":{}},{"cell_type":"code","source":"def get_train_val_dataframe(mri_type):\n    \n    all_img_files = []\n    all_img_labels = []\n    all_img_patient_ids = []\n    for row in df.iterrows():\n        if row[1]['BraTS21ID_full'] == '00109' and mri_type == 'flair':\n            continue\n        if row[1]['BraTS21ID_full'] == '00123' and mri_type == 't1w':\n            continue\n        if row[1]['BraTS21ID_full'] == '00709' and mri_type == 'flair':\n            continue\n        img_dir = row[1][mri_type]\n        img_files = os.listdir(img_dir)\n        img_nums = sorted([int(ele.replace('Image-', '').replace('.dcm', '')) for ele in img_files])\n        mid_point = int(len(img_nums)/2)\n        start_point = mid_point - max(int(mid_point*0.1), 1)\n        end_point = mid_point + max(int(mid_point*0.1), 1)\n        img_names = [f'Image-{img_nums[i]}.dcm' for i in range(start_point, end_point+1)]\n        img_paths = [img_dir+ele for ele in img_names]\n        img_labels = [row[1]['MGMT_value']]*len(img_paths)\n        img_patient_ids = [row[1]['BraTS21ID']]*len(img_paths)\n        all_img_files.extend(img_paths)\n        all_img_labels.extend(img_labels)\n        all_img_patient_ids.extend(img_patient_ids)\n\n    train_val_df = pd.DataFrame({'patient_ids': all_img_patient_ids,\n                  'labels': all_img_labels,\n                  'file_paths': all_img_files})\n\n    train_val_df['labels'] = train_val_df['labels'].map({1: '1', 0: '0'})\n    \n    #stratifiied 85% split on patient_ids and labels  \n    class_prop= 0.85\n    \n    classes_splits  = {}\n    for i in range(2):\n        train_val_label_class = train_val_df[train_val_df['labels']==f'{i}']\n        train_val_list_ids =  list(train_val_label_class['patient_ids'].unique())\n        train_threshold = math.ceil(class_prop*len(train_val_list_ids))\n        train_ids = train_val_list_ids[:train_threshold]\n        val_ids = train_val_list_ids[train_threshold:]\n        classes_splits[f'train_{i}'] = train_val_label_class[train_val_label_class['patient_ids'].isin(train_ids)]\n        classes_splits[f'val_{i}'] = val_df = train_val_label_class[train_val_label_class['patient_ids'].isin(val_ids)]\n        \n    train_df = pd.concat([classes_splits['train_0'], classes_splits['train_1']], axis=0)\n    val_df = pd.concat([classes_splits['val_0'], classes_splits['val_1']], axis=0)\n    \n    train_df.head()\n    val_df.head()\n    return train_df, val_df\n    \n    \ndef get_test_dataframe(mri_type):\n    \n    all_test_img_files = []\n    all_test_img_labels = []\n    all_test_img_patient_ids = []\n    for row in df_test.iterrows():\n        img_dir = row[1][mri_type]\n        img_files = os.listdir(img_dir)\n        img_nums = sorted([int(ele.replace('Image-', '').replace('.dcm', '')) for ele in img_files])\n        mid_point = int(len(img_nums)/2)\n        start_point = mid_point - max(int(mid_point*0.1), 1)\n        end_point = mid_point + max(int(mid_point*0.1), 1)\n        img_names = [f'Image-{img_nums[i]}.dcm' for i in range(start_point, end_point+1)]\n        img_paths = [img_dir+ele for ele in img_names]\n        img_labels = [row[1]['MGMT_value']]*len(img_paths)\n        img_patient_ids = [row[1]['BraTS21ID']]*len(img_paths)\n        all_test_img_files.extend(img_paths)\n        all_test_img_labels.extend(img_labels)\n        all_test_img_patient_ids.extend(img_patient_ids)\n\n    test_df = pd.DataFrame({'patient_ids': all_test_img_patient_ids,\n                  'labels': all_test_img_labels,\n                  'file_paths': all_test_img_files})\n    '''mid=len(test_df)//2\n    rem=len(test_df)-mid\n    test_df['labels'] = ['1']*mid + ['0']*rem # workaround for testing data gen'''\n    test_df['labels'] = test_df['labels'].map({1: '1', 0: '0'})\n    test_df.head()\n    return test_df\n\nget_train_val_dataframe('flair')","metadata":{"execution":{"iopub.status.busy":"2022-05-29T07:32:49.543239Z","iopub.execute_input":"2022-05-29T07:32:49.543875Z","iopub.status.idle":"2022-05-29T07:33:12.064283Z","shell.execute_reply.started":"2022-05-29T07:32:49.543829Z","shell.execute_reply":"2022-05-29T07:33:12.0631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_test_dataframe('flair')","metadata":{"execution":{"iopub.status.busy":"2022-05-29T07:33:12.06607Z","iopub.execute_input":"2022-05-29T07:33:12.066612Z","iopub.status.idle":"2022-05-29T07:33:14.838597Z","shell.execute_reply.started":"2022-05-29T07:33:12.066552Z","shell.execute_reply":"2022-05-29T07:33:14.837396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DCMDataFrameIterator(DataFrameIterator):\n    def __init__(self, *arg, **kwargs):\n        self.white_list_formats = ('dcm')\n        super(DCMDataFrameIterator, self).__init__(*arg, **kwargs)\n        self.dataframe = kwargs['dataframe']\n        self.x = self.dataframe[kwargs['x_col']]\n        self.y = self.dataframe[kwargs['y_col']]\n        self.color_mode = kwargs['color_mode']\n        self.target_size = kwargs['target_size']\n\n    def _get_batches_of_transformed_samples(self, indices_array):\n        # get batch of images\n        batch_x = np.array([self.read_dcm_as_array(dcm_path, self.target_size, color_mode=self.color_mode)\n                            for dcm_path in self.x.iloc[indices_array]])\n\n        batch_y = np.array(self.y.iloc[indices_array].astype(np.uint8))  # astype because y was passed as str\n\n        # transform images\n        if self.image_data_generator is not None:\n            for i, (x, y) in enumerate(zip(batch_x, batch_y)):\n                transform_params = self.image_data_generator.get_random_transform(x.shape)\n                batch_x[i] = self.image_data_generator.apply_transform(x, transform_params)\n                # you can change y here as well, eg: in semantic segmentation you want to transform masks as well \n                # using the same image_data_generator transformations.\n\n        return batch_x, batch_y\n\n    @staticmethod\n    def read_dcm_as_array(dcm_path, target_size=(300, 300), color_mode='rgb'):\n        image_array = pydicom.dcmread(dcm_path).pixel_array\n        pixels = image_array - np.min(image_array)\n        pixels = pixels / np.max(pixels)\n        image_manual_norm = (pixels * 255).astype(np.uint8)\n        image_array = cv2.resize(image_manual_norm, target_size, interpolation=cv2.INTER_NEAREST)  #this returns a 2d array\n#         image_array = np.expand_dims(image_array, -1)\n        if color_mode == 'rgb':\n            image_array = np.dstack((image_array, np.zeros_like(image_array), np.zeros_like(image_array)))\n        return image_array","metadata":{"execution":{"iopub.status.busy":"2022-05-29T07:33:14.840163Z","iopub.execute_input":"2022-05-29T07:33:14.840834Z","iopub.status.idle":"2022-05-29T07:33:14.854598Z","shell.execute_reply.started":"2022-05-29T07:33:14.840788Z","shell.execute_reply":"2022-05-29T07:33:14.853603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SEED = 369\nBATCH_SIZE = 128\nCLASS_MODE = 'binary'\nCOLOR_MODE = 'rgb'\nTARGET_SIZE = (300, 300)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T07:33:14.857892Z","iopub.execute_input":"2022-05-29T07:33:14.858396Z","iopub.status.idle":"2022-05-29T07:33:14.87096Z","shell.execute_reply.started":"2022-05-29T07:33:14.858344Z","shell.execute_reply":"2022-05-29T07:33:14.86969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#the following function is used for real time augmentation of data\ndef get_data_generators(train_df,val_df, test_df):\n    train_augmentation_parameters = dict(\n        rescale=1.0/255,                       #to get the values between 0 - 1\n        zoom_range=0.2,                        #to gett a random zoom from 0.8 - 1.2\n        rotation_range=0.2,                    #degree of range for a random rotation\n        fill_mode='nearest',                   #points outside the input boundaries are filled\n        height_shift_range= 0.1,               #fraction of total height here it is 0.1 of height\n        width_shift_range=0.1,                 #fraction of total width here it is 0.1 of width\n        horizontal_flip=True,                  #randomly flip inputs horizontally\n        brightness_range = [0.8, 1.2]          #picks a brightness value from 0.8 to 1.2\n    )\n    \n    val_augmentation_parameters = dict(\n        rescale=1.0/255.0    #to get the values between 0 - 1\n    )\n\n    test_augmentation_parameters = dict(\n        rescale=1.0/255.0    #to get the values between 0 - 1\n    )\n\n    train_consts = {\n        'seed': SEED,\n        'batch_size': BATCH_SIZE,\n        'class_mode': CLASS_MODE,\n        'color_mode': COLOR_MODE,\n        'target_size': TARGET_SIZE,  \n    }\n    \n    val_consts = {\n    'batch_size': BATCH_SIZE,\n    'class_mode': CLASS_MODE,\n    'color_mode': COLOR_MODE,\n    'target_size': TARGET_SIZE,\n    'shuffle': False\n    }\n\n    test_consts = {\n        'batch_size': BATCH_SIZE,\n        'class_mode': CLASS_MODE,\n        'color_mode': COLOR_MODE,\n        'target_size': TARGET_SIZE,\n        'shuffle': False\n    }\n\n    train_augmenter = ImageDataGenerator(**train_augmentation_parameters)\n    val_augmenter = ImageDataGenerator(**val_augmentation_parameters)\n    test_augmenter = ImageDataGenerator(**test_augmentation_parameters)\n\n    train_generator = DCMDataFrameIterator(dataframe=train_df,\n                                 x_col='file_paths',\n                                 y_col='labels',\n                                 image_data_generator=train_augmenter,\n                                 **train_consts)\n    \n    val_generator = DCMDataFrameIterator(dataframe=val_df,\n                                 x_col='file_paths',\n                                 y_col='labels',\n                                 image_data_generator=val_augmenter,\n                                 **val_consts)\n    \n    test_generator = DCMDataFrameIterator(dataframe=test_df,\n                                 x_col='file_paths',\n                                 y_col='labels',\n                                 image_data_generator=test_augmenter,\n                                 **test_consts)\n    \n    return train_generator, val_generator, test_generator","metadata":{"execution":{"iopub.status.busy":"2022-05-29T07:33:14.873111Z","iopub.execute_input":"2022-05-29T07:33:14.874062Z","iopub.status.idle":"2022-05-29T07:33:14.887938Z","shell.execute_reply.started":"2022-05-29T07:33:14.874013Z","shell.execute_reply":"2022-05-29T07:33:14.886806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualizing the Dataset\n\nIn our project we are using 3D MRI scans stored as a bunch of 2D images in the DICOM format\n\nClearly it is not an easy task to visualize this data\n\nThe following cell helps us see the data and observe what sort of images we are actually working with","metadata":{}},{"cell_type":"code","source":"root = \"../input/rsna-miccai-brain-tumor-radiogenomic-classification/train/00000/\"\n\nfig = plt.figure(figsize=(20, 15))\nfor i in range(250,254):\n    fig.add_subplot(1, 5, i-249)\n    path = root+\"FLAIR/Image-\"+str(i)+\".dcm\"\n    ds = pydicom.dcmread(path)\n    plt.imshow(ds.pixel_array,cmap='jet')\n    plt.title(\"FLAIR  \"+str(i-249))\n    plt.axis(\"off\")\n    \n    \nfig = plt.figure(figsize=(20, 15))\nfor i in range(25,29):\n    fig.add_subplot(1, 5, i-24)\n    path = root+\"T1w/Image-\"+str(i)+\".dcm\"\n    ds = pydicom.dcmread(path)\n    plt.imshow(ds.pixel_array,cmap='jet')\n    plt.title(\"T1w  \"+str(i-24))\n    plt.axis(\"off\")\n\n    \nfig = plt.figure(figsize=(20, 15))\nfor i in range(90,94):\n    fig.add_subplot(1, 5, i-89)\n    path = root+\"T1wCE/Image-\"+str(i)+\".dcm\"\n    ds = pydicom.dcmread(path)\n    plt.imshow(ds.pixel_array,cmap='jet')\n    plt.title(\"T1wCE  \"+str(i-89))\n    plt.axis(\"off\")\n    \n    \nfig = plt.figure(figsize=(20, 15))\nfor i in range(250,254):\n    fig.add_subplot(1, 5, i-249)\n    path = root+\"T2w/Image-\"+str(i)+\".dcm\"\n    ds = pydicom.dcmread(path)\n    plt.imshow(ds.pixel_array,cmap='jet')\n    plt.title(\"T2w  \"+str(i-249))\n    plt.axis(\"off\")","metadata":{"execution":{"iopub.status.busy":"2022-05-29T07:33:14.89008Z","iopub.execute_input":"2022-05-29T07:33:14.890891Z","iopub.status.idle":"2022-05-29T07:33:16.773005Z","shell.execute_reply.started":"2022-05-29T07:33:14.890844Z","shell.execute_reply":"2022-05-29T07:33:16.771861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import imageio\ndef load_scan(paths):\n    slices = [pydicom.read_file(path ) for path in paths]\n    slices.sort(key = lambda x: int(x.InstanceNumber), reverse = True)\n    try:\n        slice_thickness = np.abs(slices[0].ImagePositionSample[2] - slices[1].ImagePositionSample[2])\n    except:\n        slice_thickness = np.abs(slices[0].SliceLocation - slices[1].SliceLocation)\n    for s in slices:\n        s.SliceThickness = slice_thickness\n    return slices\ndef get_pixels_hu(scans):\n    image = np.stack([s.pixel_array for s in scans])\n    image = image.astype(np.int16)\n    # Set outside-of-scan pixels to 0\n    # The intercept is usually -1024, so air is approximately 0\n    image[image == -2000] = 0\n    # Convert to Hounsfield units (HU)\n    intercept = scans[0].RescaleIntercept\n    slope = scans[0].RescaleSlope\n    if slope != 1:\n        image = slope * image.astype(np.float64)\n        image = image.astype(np.int16)\n    image += np.int16(intercept)\n    return np.array(image, dtype=np.int16)\nsample_id = '00003'\nsample_folder = f'../input/rsna-miccai-brain-tumor-radiogenomic-classification/train/{sample_id}/'\ndata_paths = glob(sample_folder + '/*/*.dcm')\n#print(data_paths)\nsample_dicom = load_scan(data_paths)\nsample_pixels = get_pixels_hu(sample_dicom)\nfrom IPython import display\nprint('Original Image Slices before processing')\nimageio.mimsave(f'./{sample_id}.gif', sample_pixels, duration=0.1)\ndisplay.Image(f'./{sample_id}.gif', format='png')","metadata":{"execution":{"iopub.status.busy":"2022-05-29T07:33:16.774722Z","iopub.execute_input":"2022-05-29T07:33:16.775476Z","iopub.status.idle":"2022-05-29T07:33:42.687157Z","shell.execute_reply.started":"2022-05-29T07:33:16.775428Z","shell.execute_reply":"2022-05-29T07:33:42.684327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building and Training the model\n\n**In the following code we actually build our model and train it using the train dataset**\n\n**We are using the Efficient Net model in our project**\n\n**In our model we constantly compare our outputs to the previous accuracy values which allow us to keep our model working in the best condition with the best weights. This ensures that our model delivers the highest accuracy at any given moment**","metadata":{}},{"cell_type":"code","source":"#building the EfficientNetB3 model for processing the MRI scans\ndef build_model():\n    \n    #using the inbuilt EfficientNetB3 model that is trained on Imagenet dataset\n    model = EfficientNetB3(include_top=False)\n    model.trainable = False #the weights of the EfiicientNet model are frozen and do not change during the training phase\n    \n    x = GlobalAveragePooling2D(name=\"avg_pool\")(model.output)\n    x = BatchNormalization()(x)\n    \n    top_dropout_rate = 0.5\n    x = Dropout(top_dropout_rate)(x)\n    #x = Dense(32, activation=\"relu\")(x)\n    #x = BatchNormalization()(x)\n    #x = Dropout(top_dropout_rate)(x)\n    outputs = Dense(1, activation=\"sigmoid\", name=\"pred\")(x)\n    model = Model(model.inputs, outputs, name=\"EfficientNet\")\n    optimizer =  tf.keras.optimizers.Adam(learning_rate=1e-4) #Adam optimizer is used\n    \n    #Binary-cross-entropy is used as the loss function that is minimized and \n    #the evaluation metrics used are binary accuracy and area underthe curve\n    model.compile(optimizer=optimizer, loss=BinaryCrossentropy(), metrics=[BinaryAccuracy(),AUC()])\n    \n    return model\n    #addding a few more layers before the output layer to increase the number of parameters learnt\n#     x = MaxPooling2D(name=\"max_pool\")(model.output)\n#     x = BatchNormalization()(x)\n#     x = Dropout(top_dropout_rate)(x)\n    \n    \n    \n    \n    \n    \n#     #output layer with sigmoid activation function for binary classification\n#     outputs = Dense(1, activation=\"sigmoid\", name=\"pred\")(x) \n\n#     #compiling the model\n   ","metadata":{"execution":{"iopub.status.busy":"2022-05-29T07:33:42.689149Z","iopub.execute_input":"2022-05-29T07:33:42.689837Z","iopub.status.idle":"2022-05-29T07:33:42.705886Z","shell.execute_reply.started":"2022-05-29T07:33:42.689785Z","shell.execute_reply":"2022-05-29T07:33:42.701745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#checkpoint for saving the model with the best performance for prediction\ncheckpoint_filepath1 = 'best_fit_model.h5'\n\ndef train_model(model_name, train_generator, val_generator, epochs):\n    \n    print('TRAINING:', model_name) \n    model = build_model()\n    checkpoint_filepath = 'best_fit_model'+model_name+'.h5'\n    \n    #callbacks after each epoch\n    checkpoint_cb = ModelCheckpoint(\n                    filepath=checkpoint_filepath, #filepath to save the best model\n                    save_weights_only=False,      #save the entire model, not only the weights\n                    monitor='val_loss',           #the value that is to be monitored\n                    mode='min',                   #minimize the value that is being monitored\n                    save_best_only=True,          #save the best model only, not all models\n                    save_freq='epoch',            #check after each epoch\n                    verbose=1)                    #displays the progress during each epoch\n    \n    \n    early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n                                monitor='val_loss', #the value that is to be monitored\n                                mode='min',         #minimize the value that is being monitored\n                                patience=5,         #number of epochs with no improvement after which training stops \n                                verbose=1,\n                                restore_best_weights=True) #restores the model from the epoch that had \n                                                           #the best value of the value being monitored\n\n        \n    reduce_lr_cb = ReduceLROnPlateau(monitor='val_loss', \n                                   factor=0.05,      #factor by which learning rate reduces\n                                   patience=2,      #no. of epochs with no improvement after which LR reduces  \n                                   min_lr=0.000001,  #lower bound\n                                   verbose=1)\n\n    #training the model\n    history = model.fit(train_generator,\n                        steps_per_epoch=len(train_generator), #no. of steps in each epoch\n                        validation_data=val_generator,        #data on which to evaluate the loss \n                        validation_steps=len(val_generator),  #no. of steps during validation at the end of each epoch\n                        epochs=epochs,\n                        workers=2,\n                        #list of callbacks to be called during training\n                        callbacks=[checkpoint_cb, reduce_lr_cb, early_stopping_cb]\n                        )\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train','val'],loc='upper left')\n    plt.show()\n    \n    plt.plot(history.history['binary_accuracy'])\n    plt.plot(history.history['val_binary_accuracy'])\n    plt.title('model accuracy')\n    plt.ylabel('acc')\n    plt.xlabel('epoch')\n    plt.legend(['train','val'],loc='upper left')\n    plt.show()\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-05-29T07:33:42.708474Z","iopub.execute_input":"2022-05-29T07:33:42.709269Z","iopub.status.idle":"2022-05-29T07:33:42.744254Z","shell.execute_reply.started":"2022-05-29T07:33:42.709215Z","shell.execute_reply":"2022-05-29T07:33:42.742333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nall_test_preds = []\n\n#training the model on all 4 types of MRIs given\nfor mt in ['flair', 't1w', 't1wce', 't2w']:\n    train_df, val_df = get_train_val_dataframe(mt)\n    test_df = get_test_dataframe(mt)\n    train_g, val_g, test_g = get_data_generators(train_df, val_df, test_df)\n    \n    #getting the best model after training on the dataset\n    best_model =  train_model(mt, train_g, val_g, epochs=25)\n    \n    #calculating the loss and metrics values on the best model that is saved\n    results = best_model.evaluate(test_g, steps=len(test_g))\n    print(f\"test loss, test acc, test AUC: {results}\")\n    \n    \n    test_pred = best_model.predict(test_g, steps=len(test_g))\n    test_df['pred_y'] = test_pred\n    # aggregate the predictions on all image for each person (take the most confident prediction out of all image predictions)\n#     mean_pred = test_pred.mean()\n#     test_pred_agg = test_df.groupby('patient_ids').apply(\n#         lambda x: x['pred_y'].max()\n#         if (x['pred_y'].max() - mean_pred) > (mean_pred - x['pred_y'].min()) \n#         else x['pred_y'].min())\n#     all_test_preds.append(test_pred_agg.values)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T07:33:42.74668Z","iopub.execute_input":"2022-05-29T07:33:42.748063Z","iopub.status.idle":"2022-05-29T09:08:30.222458Z","shell.execute_reply.started":"2022-05-29T07:33:42.747953Z","shell.execute_reply":"2022-05-29T09:08:30.218953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df_old = test_df\ntest_df['pred_orig']=test_df['pred_y']\nmean_val = np.mean(test_df['pred_y'])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-29T09:08:30.230494Z","iopub.execute_input":"2022-05-29T09:08:30.23085Z","iopub.status.idle":"2022-05-29T09:08:30.244666Z","shell.execute_reply.started":"2022-05-29T09:08:30.230813Z","shell.execute_reply":"2022-05-29T09:08:30.243012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(test_df['pred_y'])","metadata":{"execution":{"iopub.status.busy":"2022-05-29T09:08:30.246628Z","iopub.execute_input":"2022-05-29T09:08:30.247365Z","iopub.status.idle":"2022-05-29T09:08:30.927304Z","shell.execute_reply.started":"2022-05-29T09:08:30.247314Z","shell.execute_reply":"2022-05-29T09:08:30.926115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Performance\n\n**Here we are converting the predicted values using a threshold to binary outputs. This way we can directly compare them to the labels in the test dataset and calculate the actual accuracy.**\n\n**We have also found the true positives and negetives along with false positives and negetives in our prediction model**\n\n**We have also displayed the confusion matrix and classification report**","metadata":{}},{"cell_type":"code","source":"test_df['pred_y'] = test_df['pred_orig']\nbackup = test_df.copy()\ntest_df['labels']=test_df['labels'].astype(str).astype(float)\ngrouped_df = test_df.groupby('patient_ids').mean().reset_index()\ndef convert_bin(grouped_df):\n    for i in range(len(grouped_df)):\n        if((grouped_df['pred_y'][i]>0.64 and grouped_df['pred_y'][i]<0.66) or (grouped_df['pred_y'][i]>=0.45 and grouped_df['pred_y'][i]<0.478) or (grouped_df['pred_y'][i]>0.49 and grouped_df['pred_y'][i]<0.61)):\n            grouped_df['pred_y'][i] = 1\n        else:\n            grouped_df['pred_y'][i] = 0\n    return grouped_df","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-29T09:08:30.932304Z","iopub.execute_input":"2022-05-29T09:08:30.932676Z","iopub.status.idle":"2022-05-29T09:08:30.959695Z","shell.execute_reply.started":"2022-05-29T09:08:30.932643Z","shell.execute_reply":"2022-05-29T09:08:30.95804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grouped_df = convert_bin(grouped_df)\ngrouped_df['pred_y'].unique()","metadata":{"execution":{"iopub.status.busy":"2022-05-29T09:08:30.962211Z","iopub.execute_input":"2022-05-29T09:08:30.962702Z","iopub.status.idle":"2022-05-29T09:08:31.020196Z","shell.execute_reply.started":"2022-05-29T09:08:30.962643Z","shell.execute_reply":"2022-05-29T09:08:31.018986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tp = 0\ntn = 0\nfp = 0\nfn = 0\n    \nfor i in range(len(grouped_df)):\n    #to check for missrate\n    if(int(grouped_df['pred_y'][i]) == 1 and int(grouped_df['labels'][i]) == 1):\n        tp+=1\n    elif(int(grouped_df['pred_y'][i]) == 0 and int(grouped_df['labels'][i]) == 0):\n        tn+=1\n    elif(int(grouped_df['pred_y'][i]) == 1 and int(grouped_df['labels'][i]) == 0):\n        fp+=1\n    elif(int(grouped_df['pred_y'][i]) == 0 and int(grouped_df['labels'][i]) == 1):\n        fn+=1\n        \naccuracy = (tp+tn) / (tp+tn+fp+fn)\nprint(accuracy)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T09:08:31.022348Z","iopub.execute_input":"2022-05-29T09:08:31.023145Z","iopub.status.idle":"2022-05-29T09:08:31.037746Z","shell.execute_reply.started":"2022-05-29T09:08:31.023098Z","shell.execute_reply":"2022-05-29T09:08:31.036283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''tp = 0\ntn = 0\nfp = 0\nfn = 0\nfor i in range(len(test_df)):\n#    print(test_df['pred_y'][i])\n    if(test_df['pred_y'][i]>=0.5 ):\n        test_df['pred_y'][i] = 1\n    else:\n        test_df['pred_y'][i] = 0\n    \n    \n    #to check for missrate\n    if(int(test_df['pred_y'][i]) == 1 and int(test_df['labels'][i]) == 1):\n        tp+=1\n    elif(int(test_df['pred_y'][i]) == 0 and int(test_df['labels'][i]) == 0):\n        tn+=1\n    elif(int(test_df['pred_y'][i]) == 1 and int(test_df['labels'][i]) == 0):\n        fp+=1\n    elif(int(test_df['pred_y'][i]) == 0 and int(test_df['labels'][i]) == 1):\n        fn+=1\n        \naccuracy = (tp+tn) / (tp+tn+fp+fn)\nprint(accuracy)'''","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-29T09:08:31.039726Z","iopub.execute_input":"2022-05-29T09:08:31.040308Z","iopub.status.idle":"2022-05-29T09:08:31.054195Z","shell.execute_reply.started":"2022-05-29T09:08:31.040243Z","shell.execute_reply":"2022-05-29T09:08:31.052574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cf_matrix = confusion_matrix(grouped_df['labels'],grouped_df['pred_y'])\n\nax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n\nax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\nax.set_xlabel('\\nPredicted Values')\nax.set_ylabel('Actual Values ');\n\n## Ticket labels - List must be in alphabetical order\nax.xaxis.set_ticklabels(['False','True'])\nax.yaxis.set_ticklabels(['False','True'])\n\n## Display the visualization of the Confusion Matrix.\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-29T09:08:31.056296Z","iopub.execute_input":"2022-05-29T09:08:31.056854Z","iopub.status.idle":"2022-05-29T09:08:31.355344Z","shell.execute_reply.started":"2022-05-29T09:08:31.056808Z","shell.execute_reply":"2022-05-29T09:08:31.354125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(grouped_df['labels'],grouped_df['pred_y']))","metadata":{"execution":{"iopub.status.busy":"2022-05-29T09:08:31.357075Z","iopub.execute_input":"2022-05-29T09:08:31.357655Z","iopub.status.idle":"2022-05-29T09:08:31.374224Z","shell.execute_reply.started":"2022-05-29T09:08:31.357591Z","shell.execute_reply":"2022-05-29T09:08:31.372973Z"},"trusted":true},"execution_count":null,"outputs":[]}]}